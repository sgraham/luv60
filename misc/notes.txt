Trivial jmp skipping is wrong when:

    0: jz <A>    ; pending continuation A
  ...: ...
    5: jmp <B>   ; pending continuation B

  - resolve <A> to next instr at 6: happily patches A to point at 6

  - resolve <B> to 6:
    realizes 5 is a `jmp rel 0`, rewinds output location to 5 (overwriting
    useless jmp), but now the jz is pointing past the block.

Could try to hack and slash (re-fix-up, defer fixups another layer, etc.)

But it seems like gen might need to build a graph of snip calls and their
connections, which seems like it might be pretty much a cps graph, or a post
order walk bytecode again.

--------------------------------------------------------------------------------

if 2:
    return 3
return 123

GEN: "init"
GEN: "func_entry"
GEN: "push_number"
GEN: "if"
PATCH: patching if_cond to 0xE
PATCH:  fixup at 0xA
PATCH: patching if_then to 0x1C
PATCH:  fixup at 0x18
GEN: "push_number"
GEN: "func_return"
PATCH: patching return_result to 0x26
PATCH:  fixup at 0x22
GEN: "jump"
PATCH: patching if_else to 0x33
PATCH:  fixup at 0x13
PATCH: patching if_after to 0x33
PATCH:  fixup at 0x2F
GEN: "push_number"
GEN: "func_return"
PATCH: patching return_result to 0x3D
PATCH:  fixup at 0x39
GEN: "jump"
PATCH: patching end_of_function to 0x4A
PATCH:  fixup at 0x46
GEN: "finish_and_dump"

  2           0 LOAD_GLOBAL              0 (x)
              2 POP_JUMP_IF_FALSE        8

  3           4 LOAD_CONST               1 (3)
              6 RETURN_VALUE

  4     >>    8 LOAD_CONST               2 (123)
             10 RETURN_VALUE


---

InstrData instrs[1<<20]
type Instr = u32

BlockData blocks[1<<20]
type Block = u32

BlockData:
  Phi phi
  Instr instr_start
  int num_instrs



---

I'm not sure if it's sensible to add interpolated Swift-style strings (or some
other style). The lexer indexing currently ignores \().

Adding ` to be like " also sort of works in the indexer. So the syntax could "x
+ y is `x+y`." which isn't too bad looking (sort of like shell). The first token
would be QUOTED though, and so the parser can't easily tell if it should be
building an interpolation list without looking ahead, and then if there's
expressions inside the escape it's arbitraily far to know if you're ever going
to get to a `. So that doesn't seem to work?

Sticking with \(), the parser has to some how fully parse/lex the string as it's
own little program. It won't be able to call parse_expression() to get the sub
things otherwise as the whole thing is one big STRING_QUOTED. Maybe that's the
way to go to keep the normal (non-interpolated) case fast. Maybe a memchr for
the escape character ($, `, #, whatever) to know if the slow interpolate re-lex is
required too (since those can't appear in actual code).



---

not really achieving luajit speed. lj exec is ~4.85s, with ir_jit_compile
removed (so only building ir), ~4.54s. when actually building code, in the
~20-25s range.

need to check some basics, is categorizing tokens killing it? maybe going back
to that constantly is a bad tradeoff to make lex indexing seem faster than it
is.

alternatively, need a faster way to parse. is it bad code or bad lang design?
there's no lookahead right now.

things that stand out in profile:
- dict ops (VirtualAlloc was esp bad)
- heavily used though -- str_intern, var scope. maybe a smaller non-allocating
  one that can be used until it grows past a certain point
- ir_init mallocs, generally redirect everything to bump arena?
- make a set of stub ir_AAAA to get real no-op time. i think they just have to
  all just either return 0 or void

maybe it's time to dump py syntax for something faster to parse. are braces or
ends actually necessarily faster?

what about more drastic things? like what if the input was already a flatbuffer,
how fast could it go?

- bump arena, never malloc/virtualalloc
- ir_STUB header for parse_code_gen.c and parse_syntax_check.c
- faster symtab for smaller scopes
- cur_func->ctx indirects maybe better to copy on enter/leave
- categorizer could be multicore since buffer is 1:1 now
- parser restructure, maybe Carbon had some idea? not sure if different than
  standard LR(n) table driven
- get rid of pratt and use manual precedence for expressions (read lj_parse more
  carefully)


hyperfine "out\wr\luvc.exe out\wr\dumbbench.luv"

ir_jit_compile #if 0 (but still building ir_XXXX)

initial:
  Time (mean ± σ):      7.051 s ±  0.070 s    [User: 3.631 s, System: 3.392 s]
most things to Arena:
  Time (mean ± σ):      3.042 s ±  0.033 s    [User: 2.742 s, System: 0.284 s]
ir to Arena:
  Time (mean ± σ):      2.729 s ±  0.020 s    [User: 2.448 s, System: 0.269 s]
--syntax-only (stub all ir_):
  Time (mean ± σ):      1.587 s ±  0.015 s    [User: 1.354 s, System: 0.210 s]
back to virtualalloc for main file/token_offsets:
  Time (mean ± σ):      1.546 s ±  0.016 s    [User: 1.359 s, System: 0.172 s]
flat map for func syms instead of full swisstable:
  Time (mean ± σ):      1.461 s ±  0.013 s    [User: 1.301 s, System: 0.147 s]
new short str impl
  Time (mean ± σ):      1.013 s ±  0.014 s    [User: 0.887 s, System: 0.109 s]

damn, m3 macbook air on battery for --syntax-only is <1s, amazing at branchy code
  Time (mean ± σ):     990.3 ms ±   6.4 ms    [User: 916.7 ms, System: 67.2 ms]
flat map might be a bit slower on mac? strange
  Time (mean ± σ):     999.0 ms ±   3.9 ms    [User: 931.5 ms, System: 61.6 ms]
hmm, fiddling wiht the module dict size (1<<16 vs 1<<24) mildly affects it, weirdness seems opposite to windows too (where rehash is worse than initial clear)
  Time (mean ± σ):     961.4 ms ±   2.8 ms    [User: 887.5 ms, System: 66.9 ms]
new short str impl
  Time (mean ± σ):     795.1 ms ±   9.7 ms    [User: 730.1 ms, System: 54.8 ms]

Actual compilation is 25-30s:
a lot of time in memory futzing again, probably giving IR a slice of a buffer
would be a lot better.
with code_buffer pre-alloc, --opt 1
win 3900x:
  Time (mean ± σ):      7.306 s ±  0.056 s    [User: 7.094 s, System: 0.186 s]
mac m3a:
  Time (mean ± σ):      4.747 s ±  0.033 s    [User: 4.639 s, System: 0.091 s]

no ir_check in release:
  Time (mean ± σ):      6.869 s ±  0.050 s    [User: 6.673 s, System: 0.173 s]

get rid of cstr mallocs (and don't pass name for var in debug):
win:
  Time (mean ± σ):      6.706 s ±  0.071 s    [User: 6.522 s, System: 0.167 s]
mac:
  Time (mean ± σ):      4.384 s ±  0.029 s    [User: 4.284 s, System: 0.079 s]




---

struct default and compound initializers

- when making each struct, either memset 0, or memcpy blob

struct SubThing:
    int a
    int b

struct Stuff:
    SubThing thing = SubThing(1, 2)
    bool b = false

==> Type needs a pointer to a blob filled out (slightly convoluted to build
because we don't have the offsets until after the type is built?)

==> need to handle compound literals in the value of struct initializers, but
is_const only

--- todo list

- [x] function hoisting: not most impt, but harder in single pass
- [ ] list compr: also messy-ish because of late binding of it
- [ ] more intricate scope resolution (esp for nested)
- [ ] slices
- [ ] dict
- [ ] somewhere in here, need to do struct passing in IR
- [ ] pointers
- [ ] const
- [ ] memfns
- [ ] to_str, and auto for print
- [ ] arena helpers
- [ ] break/continue
- [ ] loop over other things
- [ ] in/not in
- [ ] revisit string interpolation; don't slow down lex/parse if unused
- [ ] aggregate upvals


--- upval handling

single pass makes function hoisting and upvals a bit harder than ast

the scope_lookup in parse_variable() can know that the value is an upval because
that's a lexical thing.

need to assume that any nested function definition has some upvals i think
because we need to define the nested function as taking a hidden upval param
pointer at enter_function time.

when a variable is determined to be an upval while parsing the body,
find/allocate space in the current function's flattened capture blob. references
to upvals are always through $up[N]. allocating space has to point at the local
so it knows what to capture

it can't know how big the blob is until the end of the function. but to avoid
needing to have callers inject a hidden argument, could indirect at entry. so!
on enter_function nested, alloca a pointer that points at the capture blob (in
the outside function). the first thing a nested function does is load its
capture pointer to a local ref, and then all capture references are fixed
offsets from that base.

at the leave_function(), alloca the full size of the capture blob, and copy all
referenced upvals into the associated slot. stash the value of the alloca into
the pointer allocated at the beginning enter_function and then the prolog to the
nested function just loads that.

... that makes no sense of course, having a fixed pointer allocated per function
is dumb because the function can't recurse, amongst other problems.

so:
- any nested function needs to have a hidden void* argument because we can't
tell at enter_function() time if it's going to need upvals. (i think the caller
can just pass null if it turns out there are zero upvals, so not a huge cost)
- upval refs are always compiled as up[N] in the body
- need a NESTED flag in the function type to know that it needs an upval pointer
  at call time
- in leave_function(), alloca the block, copy all referenced upvals into it
- calls to NESTED need to put the alloca pointer as the first op in
parse_call().
- not sure about function pointers yet

don't try to solve escape or closure, potentially some sort of arena integration
some day.


---

list comprehension parsing

from parse_list_literal_or_compr():

Parsing this expression will fail in a list comprehension because the
expression won't be able to lookup the identifiers that are bound in
the `for` clause. Not sure how to best resolve this, I'm tempted to put
the for at the beginning, even though it will be annoyingly flipped
from Python, i.e.:

   result = [for x in nums: x*3]

rather than:

   result = [x*3 for x in nums]

It feels randomly different, but since it should make the parser
not-crazy here, it's probably the right way to go?

[for x in nums: if x % 2 == 0: x*3]

This is annoying that it makes a new category of expression-after-for/if without
a return.

So the other way is some way to find the for. A 'skim' parse_expression would be
full way where: it doesn't do var lookup or any IR until it's finished the expr.
then check for `for` and then we know if list literal or comprehension. but that
seems very not-suitable for the current code because it goes down the whole
pratt parser rules table, which touches a lot. a second rule table might be
better, but it has to match the first one's advancment, so the whole thing seems
a bit stinky.

since `for` can't appear in an expression, maybe it's fine to just peek tokens
until either TOK_FOR or TOK_RSQUARE? doesn't even have to be a real peek (in
terms of buffering the categorize) it could just scan through. i think that
mostly works, except for nested list compr, like:

  [x + [y for y in range(5)] for x in some_lists]
          ^

then the first x would find the marked one. is it enough to keep a stack of
LSQUARE/RSQUARE pairs?

==> at start of list after first [, nest_count = 0, if [ increment. if ]
decrement. if < 0 done and not-comprehension. if for, save location, compile as
compr, and then compile the expr after bindings.

probably need inc/dec on {( and }) too, if dict/set compr.

seems pretty doable in theory. don't have machinery to skim ahead like that, so
will need a version of advance() that knows how to do something like that. it
doesn't need full newline/indent handling, only NL since it's necessarily inside
[ ]. and the compilation of the expression should just be parse_expression(),
assert the location is the for, and the set the location to after the clauses.
easy peasy?


---

listener based operation

receives one top level thing at a time

builds ast for the thing

if the thing is a redefinition, dirties anything in the graph that depends on it

forall dirty in dirties:
  build_debug()
  build_opt()

debug indirects through heap-type-map which can look up type information for a
given address, and from that, dynamically get a field
when setting:
- if field type changed: print warning, do nothing
- if field removed: print warning, do nothing
- if field 

---

garbage collector for debug only
- we don't know types of arena objects because there's no object model
- but in practice we do normally know, so rely on cooperation
- something like __asan_poison(), do __impl_set_type(p, size, typeid())

---

incremental graph update

top level: [can depend on]
foreign: [struct, import]
on: [struct, import]
import: [maybe import]
struct: [struct]
def: [struct]
const: [everything]
check: [everything]

a top-level chunker, to be able to quickly diff/cache/etc blobs, in lieu of
having a smalltalk browser where each thing is separate. dirty when the sha
of the thing is different

what does the dependency look like? need to do a resolve like luv0, then
create additional links between ast nodes


# vim: set tw=80 fo=roqnjt:
